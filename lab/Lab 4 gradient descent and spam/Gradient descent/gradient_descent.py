# -*- coding: utf-8 -*-
"""gradient_descent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s678-feieT4nP5WBnVdY21gIRgHv1I1-

# Q4:b)Implement Gradient Descent algorithm in the following manner:  (Give it a try)
a. First take a dataset with single independent variable and find the best fit using Simple Linear Regression. 

b. Using the values of coefficients given in step a, apply gradient descent to minimize loss function and then make prediction again. 

c. Compare the best fit line of case a and b.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

seed = 23

df = pd.read_csv("experience_salary.csv")

df.head()

df.isna().sum()

df.describe()

X = df.iloc[:,:-1].values
y = df.iloc[:,-1].values

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed)

"""## Linear regression"""

lr = LinearRegression()
lr.fit(X_train,y_train)
y_pred = lr.predict(X_test)

mean_squared_error(y_test,y_pred)

squared_error = mean_squared_error(y_test,y_pred)*y_test.shape[0]
squared_error

y_test

y_pred

lr.coef_

"""##Gradient Descent
<font size=4></br>Gradient descent is very helpful in finding the minima of the loss function.
</br>step 1: take derivative of loss function
</br>step 2: start off with random values (generally constant is 0 while a factor is given a value of 1)
</br>step 3: now put these values into the loss function's derivative
</br>step 4: step size = result from step 3* learning rate(shouldn't be big enough that it jumps over the minima)
</br>step 5: new value for coefficients = old value-step size
</br>step 6: repeat above steps until step size becomes very small or max iteration is reached
</font>
"""

X = df.iloc[:,:1].values
y = df.iloc[:,-1:].values
X_train2,X_test2,y_train2,y_test2 = train_test_split(X,y,random_state=seed,test_size=0.2)
data = np.concatenate((X_train2,y_train2),axis=1)

from sympy import symbols,lambdify
x,y,a,b = symbols('x y a b')
lsf = (y-(a*x+b))**2
lsf_a = lsf.diff(a)
lsf_b = lsf.diff(b)
lsf_b_f = lambdify((x,y,a,b),lsf_b)
lsf_a_f = lambdify((x,y,a,b),lsf_a)
def se_calc(f,m,n,data):
    return sum([f(i,j,m,n) for i,j in data])

def gradient_descent(m,n,data,learning_rate,k,min_step):
    for i in range(k):
        step_size_a = se_calc(lsf_a_f,m,n,data)*learning_rate
        step_size_b = se_calc(lsf_b_f,m,n,data)*learning_rate
        if abs(step_size_a)<min_step and abs(step_size_b)<min_step:
            return m,n
        if abs(step_size_b)>=min_step:
            n-=step_size_b
        if abs(step_size_a)>=min_step:
            m-= step_size_a
    return m,n

m,n = gradient_descent(1,0,data,0.00001,20000,0.00005)

test_data = np.concatenate((X_test2,y_test2),axis=1)

print(m,n)

lsf_f = lambdify((x,y,a,b),lsf)
se_calc(lsf_f,m,n,test_data)

y_pred2 = list()
for i in range(X_test.shape[0]):
    y_pred2.append(m*X_test[i][0]+n)
y_pred2 = np.array(y_pred2)

mean_squared_error(y_test,y_pred2)*y_pred2.shape[0]

t = np.concatenate((np.array(list(map(lambda x: [x],list(y_pred)))),y_test2,(np.array(list(map(lambda x: [x],list(y_pred2)))))),axis=1)

t

gde = mean_squared_error(y_test,y_pred2)*y_pred2.shape[0]
lre = mean_squared_error(y_test,y_pred)*y_pred.shape[0]
print(f'gradient descent: {gde}\nlinear rigression: {lre}\ndifference(reg-grad):{lre-gde}')